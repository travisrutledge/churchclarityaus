{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Queer-Affirming vs non-Affirming Churches: A Web Scraping and Text Analysis Project\n",
    "\n",
    "**Project Description**:  \n",
    "The goal of this project is to develop a comprehensive database of churches in Australia, where each church is flagged as being queer-affirming or not. This will provide more clarity to queer individuals looking for places of worshop where they will accepted and treated equally. \n",
    "\n",
    "**Goals**:  \n",
    "1. Extract relevant textual information from church websites.  \n",
    "2. Clean and preprocess the text data for analysis.  \n",
    "3. Identify patterns, keywords, and indicators of queer-affirming vs non-affirming language.  \n",
    "4. Flag each church as 'affirming\", \"non-affirming\", or \"unknown\". \n",
    "\n",
    "**Author**:  \n",
    "Travis Rutledge, travisrutledge@gmail.com\n",
    "\n",
    "**Last Updated**:  \n",
    "12/02/2025\n",
    "\n",
    "**Notebook Outline**:  \n",
    "1. Collecting a List of Australian Churches\n",
    "2. URL Text Scraping\n",
    "2. Data Cleaning  \n",
    "3. Text Preprocessing  \n",
    "4. Sentiment Analysis  \n",
    "5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Allows to intract with operating system, such as environment variables and system commands\n",
    "import requests # Allows for http requests\n",
    "from bs4 import BeautifulSoup # HTML web scraping tool\n",
    "from urllib.parse import urljoin, urlparse # Manipulates URLs for better web scraping \n",
    "import pandas as pd # Data cleaning and transformation\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "from nltk.tokenize import word_tokenize # Separates text into individual words \n",
    "from nltk.corpus import stopwords # Filters out common words like 'the' and 'is' \n",
    "from nltk.util import ngrams # Creates bigrams, which are used for sentimental analysis\n",
    "from collections import Counter # Counts the number of unique words in each data set\n",
    "from textblob import TextBlob # Sentiment analysis \n",
    "import matplotlib.pyplot as plt # data exploration and visualisation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Converts raw text into numerical representations \n",
    "from sklearn.model_selection import train_test_split # Splits data into training (80%) and testing (20%) datasets.\n",
    "from sklearn.linear_model import LogisticRegression #A  supervised machine learning algorithm used for classification of affirming and nonaffirming\n",
    "from sklearn.metrics import classification_report, accuracy_score # Evaluates model performance with precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collecting a list of Australian Churches\n",
    "The first step is to gather a list of Australian churches, which includes their names, URL, address, and coordinates. This is done by using the Google Places API, which allows for a 1,000 search requests per month. Each request can return up to 20 places. Each query has a maximum squre range of 50km, so 50km coordinate squares will need to be determined in order to facilitate a comprehensive search of churches across Australia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API \n",
    "API_KEY = os.getenv(\"GOOGLE_PLACE_API_KEY\")\n",
    "\n",
    "# Search parameters, adjust as needed\n",
    "locations = [\n",
    "# Victoria\n",
    "#\"-37.840935,144.946457\",  # Melbourne\n",
    "#\"-36.616619,143.260361\",  # St Arnaud\n",
    "#\"-37.292870,144.951263\",  # Kilmore\n",
    "#\"-37.987461,145.214859\",  # Dandenong\n",
    "#\"-36.124428,146.876389\",  # Wodonga\n",
    "#\"-37.966667,144.133331\",  # Lethbridge\n",
    "#\"-37.823002,144.998001\",  # Richmond\n",
    "#\"-37.636604,144.806427\",  # Bulla\n",
    "#\"-36.948254,145.104477\",  # Northwood\n",
    "#\"-37.020100,145.131454\",  # Seymour\n",
    "# New South Wales\n",
    "#\"-33.865143,151.209900\",  # Sydney\n",
    "#\"-30.640720,151.500702\",  # Uralla\n",
    "#\"-34.033749,151.071198\",  # Kirrawee\n",
    "#\"-30.452242,152.897964\",  # Bellingen\n",
    "#\"-32.569473,151.178818\",  # Singleton\n",
    "#\"-33.968109,151.104080\",  # Hurstville\n",
    "#\"-30.614943,152.852127\",  # Bowraville\n",
    "#\"-34.673820,150.844376\",  # Kiama\n",
    "#\"-32.023331,151.958755\",  # Gloucester\n",
    "#\"-32.897633,151.736984\",  # Mayfield\n",
    "#\"-28.175995,153.541672\",  # Tweed Heads\n",
    "#\"-33.084999,151.634995\",  # Swansea\n",
    "#\"-34.583332,150.866669\",  # Shellharbour\n",
    "#\"-32.916668,151.750000\",  # Newcastle\n",
    "#\"-33.794498,150.976501\",  # Constitution Hill\n",
    "#\"-33.932999,151.259003\",  # South Coogee\n",
    "#\"-33.485867,149.667435\",  # Brewongle\n",
    "#\"-34.443371,150.061356\",  # Bermagui\n",
    "#\"-34.033749,151.071198\",  # Kirrawee\n",
    "# Queensland\n",
    "#\"-27.4678,153.0281\",      # Brisbane\n",
    "#\"-19.007626,146.189194\",  # Paluma\n",
    "#\"-25.898890,139.351669\",  # Birdsville\n",
    "#\"-23.439493,144.251389\",  # Longreach\n",
    "#\"-19.568516,147.406387\",  # Ayr\n",
    "#\"-27.066668,152.966660\",  # Caboolture\n",
    "#\"-20.267500,148.716949\",  # Airlie Beach\n",
    "#\"-27.616667,152.850006\",  # Collingwood Park\n",
    "#\"-27.454914,153.007126\",  # Red Hill\n",
    "#\"-26.798412,153.132965\",  # Caloundra\n",
    "#\"-27.395847,152.937881\",  # Ferny Hills\n",
    "#\"-27.585613,152.983658\",  # Durack\n",
    "#\"-27.302221,152.988815\",  # Strathpine\n",
    "\"-28.000767,153.429642\",  # Surfers Paradise\n",
    "]\n",
    "radius = 10000  # Search radius in meters\n",
    "type = [\"church\", \"place_of_worship\"] # Type of place to search for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API URLs\n",
    "base_url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "details_base_url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "\n",
    "def get_places_data(api_key, location, radius, type):\n",
    "    all_results = []  # To store all results across pages\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"location\": location,\n",
    "        \"radius\": radius,\n",
    "        \"type\": type\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the response\n",
    "        data = response.json()\n",
    "        print(f\"API Response: {data}\")  # Print full response to debug\n",
    "        all_results.extend(data.get(\"results\", []))  # Add results from the current page\n",
    "        \n",
    "        # Check if there is a next page\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break  # No more pages, exit loop\n",
    "        \n",
    "        # Wait a few seconds before using the next_page_token to avoid request denial\n",
    "        import time\n",
    "        time.sleep(5)  # Google requires a short delay before using the next page token\n",
    "        \n",
    "        print(f\"Fetched {len(data.get('results', []))} results\")\n",
    "        print(f\"Next Page Token: {next_page_token}\")\n",
    "\n",
    "        # Update params with the next_page_token\n",
    "        params.update({\"pagetoken\": next_page_token})\n",
    "    \n",
    "        if \"status\" in data and data[\"status\"] == \"OVER_QUERY_LIMIT\":\n",
    "            print(\"Google is rate-limiting you. Try again later.\")\n",
    "            break  # Stop fetching if limit is reached\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Function to get website URLs from Place Details API\n",
    "def get_place_details(api_key, place_id):\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"website\",  # Request only the website field\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    response = requests.get(details_base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        details_data = response.json()\n",
    "        return details_data.get(\"result\", {}).get(\"website\", \"N/A\")\n",
    "    else:\n",
    "        print(f\"Error fetching details for place_id {place_id}: {response.status_code}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "# Function to extract URLs\n",
    "def extract_website_urls(data, api_key):\n",
    "    websites = []\n",
    "    for result in data:\n",
    "        place_id = result.get(\"place_id\")\n",
    "        website = get_place_details(api_key, place_id)\n",
    "        if website and website != \"N/A\":\n",
    "            websites.append(website)\n",
    "    return websites\n",
    "\n",
    "# Main script to collect the list of church websites\n",
    "if __name__ == \"__main__\":\n",
    "    all_websites = []  # To store all websites from multiple locations\n",
    "\n",
    "    # Loop through locations\n",
    "    for location in locations:\n",
    "        print(f\"Fetching data for location: {location}\")\n",
    "        # Fetch data from Google Places API with pagination\n",
    "        places_data = get_places_data(API_KEY, location, radius, type)\n",
    "        \n",
    "        if places_data:\n",
    "            # Extract website URLs\n",
    "            website_urls = extract_website_urls(places_data, API_KEY)\n",
    "            all_websites.extend(website_urls)\n",
    "\n",
    "    # Remove duplicates by converting to a set and back to a list\n",
    "    all_websites = list(set(all_websites))\n",
    "    \n",
    "    # Save to a text file or use it for further steps\n",
    "    with open(\"church_websites.txt\", \"w\") as f:\n",
    "        for url in all_websites:\n",
    "            f.write(url + \"\\n\")\n",
    "    \n",
    "    print(f\"Website URLs saved to 'church_websites.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. URL Text Scraping\n",
    "Using the chuch URLs gathered in the previous step, data is scraped from each church's website. Data is broken down by website, webpage, and text content. Because each website can have numerous pages, the scraping tool only scrapes websites that are found in the top navigation of each HTML website.\n",
    "\n",
    "To Do:\n",
    "    Another web scraping tool such as Selinium or something else will need to be used to scrape from javascript websites. The current setup only scrapes HTML websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playwright Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup HMTL Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. URL Text Scraping\n",
    "# Load the list of church websites generated in Section 1\n",
    "websites = []\n",
    "\n",
    "# Read the URLs from the file created in Section 1\n",
    "with open(\"church_websites.txt\", \"r\") as f:\n",
    "    websites = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Define scraping parameters\n",
    "MAX_DEPTH = 1  # Maximum depth of scraping\n",
    "SKIP_PATTERNS = [\"tribe_events\", \"eventDisplay\", \"ical\", \"page\", \"eventDate\"]  # Patterns to skip\n",
    "\n",
    "# Normalize URL function\n",
    "from urllib.parse import urlparse, urlunparse, urljoin\n",
    "\n",
    "def normalize_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, \"\", \"\", \"\"))\n",
    "    return normalized\n",
    "\n",
    "# Function to validate URLs\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def scrape_stories(soup):\n",
    "    # Check for story-like structure\n",
    "    story_container = soup.find('div', class_='masonry-grid')  # Adjust if needed\n",
    "    if not story_container:\n",
    "        return None  # No stories found\n",
    "\n",
    "    # Extract stories\n",
    "    stories = story_container.find_all('div', class_='masonry-grid-item')\n",
    "    story_list = []\n",
    "    for story in stories:\n",
    "        title = story.find('b').get_text(strip=True) if story.find('b') else 'No Title'\n",
    "        description = story.find('i').get_text(strip=True) if story.find('i') else 'No Description'\n",
    "        link = story.find('a')['href'] if story.find('a') else 'No Link'\n",
    "        story_list.append({\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'link': link\n",
    "        })\n",
    "    return story_list\n",
    "\n",
    "def scrape_default(soup, current_url):\n",
    "    # Extract main content\n",
    "    content_div = soup.find(\"div\", {\"class\": \"main-content\"})\n",
    "    if not content_div:\n",
    "        content_div = soup.find(\"body\")  # Fallback to entire body\n",
    "    page_text = content_div.get_text(separator=\" \", strip=True) if content_div else \"\"\n",
    "\n",
    "    # Clean up text\n",
    "    page_text = page_text.replace(\"Read More\", \"\").strip()\n",
    "    return {\"URL\": current_url, \"Text\": page_text}\n",
    "\n",
    "def scrape_website(start_url, max_depth=MAX_DEPTH):\n",
    "    visited = set()  # Keep track of visited URLs\n",
    "    to_visit = [(start_url, 0)]  # Start with the homepage and depth level 0\n",
    "    data = []  # To store scraped data\n",
    "\n",
    "    while to_visit:\n",
    "        current_url, depth = to_visit.pop(0)\n",
    "\n",
    "        # Skip if the maximum depth is exceeded\n",
    "        if depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        # Skip if already visited\n",
    "        normalized_url = normalize_url(current_url)\n",
    "        if normalized_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        try:\n",
    "            # Fetch the URL\n",
    "            response = requests.get(current_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Check the content type\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "            if \"text/html\" not in content_type:\n",
    "                print(f\"Skipping non-HTML content: {current_url}\")\n",
    "                continue\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Mark as visited\n",
    "        visited.add(normalized_url)\n",
    "\n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Try scraping stories\n",
    "        stories = scrape_stories(soup)\n",
    "        if stories:\n",
    "            print(f\"Stories found on {current_url}!\")\n",
    "            for story in stories:\n",
    "                data.append({\"Website\": start_url, \"URL\": current_url, **story})\n",
    "            continue  # Skip link traversal for story pages\n",
    "\n",
    "        # Fallback to default scraping logic\n",
    "        page_data = scrape_default(soup, current_url)\n",
    "        data.append({\"Website\": start_url, **page_data})\n",
    "\n",
    "        # Extract and filter links only from navigation elements\n",
    "        nav_links = soup.find_all(['nav', 'ul'])\n",
    "        links = [urljoin(current_url, a['href']) for nav in nav_links for a in nav.find_all('a', href=True)]\n",
    "\n",
    "        for link in links:\n",
    "            normalized_link = normalize_url(link)\n",
    "            if start_url in normalized_link and normalized_link not in visited:\n",
    "                to_visit.append((link, depth + 1))  # Add the next depth level\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape each website\n",
    "if __name__ == \"__main__\":\n",
    "    all_data = []  # To store all scraped data\n",
    "\n",
    "    total_websites = len(websites)  # Get the total number of websites\n",
    "    for idx, start_url in enumerate(websites, start=1):  # Use enumerate to track the index\n",
    "        print(f\"Scraping {idx} of {total_websites} websites: {start_url}\")\n",
    "        scraped_data = scrape_website(start_url)\n",
    "        all_data.extend(scraped_data)\n",
    "\n",
    "    # Convert the scraped data to a pandas DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Validate URLs\n",
    "    df[\"URL\"] = df[\"URL\"].apply(lambda x: x if is_valid_url(x) else None)\n",
    "\n",
    "    # Drop rows where URL is invalid\n",
    "    df = df.dropna(subset=[\"URL\"])\n",
    "    print(\"Invalid URLs have been removed from the dataset.\")\n",
    "\n",
    "    # Filter out rows with empty text\n",
    "    df_cleaned = df[df[\"Text\"].notnull() & (df[\"Text\"].str.strip() != \"\")]\n",
    "    print(\"Cleaned DataFrame:\")\n",
    "    print(df_cleaned.head())\n",
    "\n",
    "    # Save cleaned data to CSV\n",
    "    df_cleaned.to_csv(\"scraped_data.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(\"Scraped data saved to 'scraped_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "This step focuses on deleting irrelevent data and correcting any inconsistencies or special characters. This code chunk lists the number of duplicate rows before and after removal, and it provides a .csv file of the cleaned data at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or empty text\n",
    "df_cleaned = df[df[\"Text\"].notnull() & (df[\"Text\"].str.strip() != \"\")]\n",
    "\n",
    "# Remove duplicates\n",
    "# Check for duplicate rows based on all columns\n",
    "print(f\"Number of duplicate rows before removal: {df.duplicated().sum()}\")\n",
    "\n",
    "# Drop duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Confirm duplicates were removed\n",
    "print(f\"Number of duplicate rows after removal: {df_cleaned.duplicated().sum()}\")\n",
    "\n",
    "# Handle encoding issues\n",
    "# Define a function to clean special characters and encoding issues\n",
    "def clean_encoding_issues(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace or remove problematic characters\n",
    "        text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        # Optionally: replace specific problematic sequences\n",
    "        text = text.replace(\"â€™\", \"'\")  # Replace right single quotation mark\n",
    "        text = text.replace(\"â€œ\", '\"')  # Replace left double quotation mark\n",
    "        text = text.replace(\"â€\", '\"')  # Replace right double quotation mark\n",
    "        text = text.replace(\"â€“\", \"-\")  # Replace en dash\n",
    "        text = text.replace(\"â€¦\", \"...\")  # Replace ellipsis\n",
    "        text = text.replace(\"â€‹\", \"\")  # Remove zero-width space\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the \"Text\" column\n",
    "df_cleaned[\"Text\"] = df_cleaned[\"Text\"].apply(clean_encoding_issues)\n",
    "\n",
    "# Save cleaned data to CSV\n",
    "df_cleaned.to_csv(\"cleaned_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Data cleaning complete. Cleaned data saved to 'cleaned_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Cleaning, Tokenising, and Bigramming\n",
    "This section sets all the text to lower case, tokenizes the text (breaking it into individual words), removes extranous words like \"this\" and \"is\", removes anything that is not a letter or a number, and generates bigrams for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all entries in the \"Text\" column are strings\n",
    "df_cleaned[\"Text\"] = df_cleaned[\"Text\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Lowercase the text\n",
    "df_cleaned[\"Text\"] = df_cleaned[\"Text\"].str.lower()\n",
    "\n",
    "# Tokenize the text (breaks the text into individual words)\n",
    "df_cleaned[\"Tokens\"] = df_cleaned[\"Text\"].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords (like 'the' and 'is')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df_cleaned[\"Tokens\"] = df_cleaned[\"Tokens\"].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
    ")\n",
    "\n",
    "# Remove anything that is not a letter or number, like punctuation and special characters\n",
    "df_cleaned[\"Tokens\"] = df_cleaned[\"Tokens\"].apply(\n",
    "    lambda tokens: [word for word in tokens if word.isalnum()]\n",
    ")\n",
    "\n",
    "# Remove illegal characters using pandas' built-in string functions\n",
    "df_cleaned = df_cleaned.map(lambda x: ''.join(filter(lambda y: y.isprintable(), str(x))) if isinstance(x, str) else x)\n",
    "\n",
    "# Generate bigrams for each text entry\n",
    "df_cleaned[\"Bigrams\"] = df_cleaned[\"Tokens\"].apply(\n",
    "    lambda tokens: list(ngrams(tokens, 2))\n",
    ")\n",
    "\n",
    "# Generate trigrams for each text entry\n",
    "df_cleaned[\"Trigrams\"] = df_cleaned[\"Tokens\"].apply(\n",
    "    lambda tokens: list(ngrams(tokens, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Keyword-Based Classification\n",
    "These are the keywords and bigrams that will be used to flag if a church is affirming or non-affirming. These are broken out by keywords, bigrams, and trigrams. A separate variable is made to flag certain denominations that are affirming or nonaffirming. FOr example, the uniting church of australia is affirming while the Australian Christian Church is nonaffirming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affirming\n",
    "affirming_keywords = [\"lgbt\", \"lgbtqia\", \"lbgtq+\", \"lgbtqia+\", \"queer\", \"bisexual\", \"lbgti\"]\n",
    "\n",
    "affirming_bigrams = [(\"sexual\", \"orientation\"), (\"gender\", \"identity\"), (\"queer\", \"affirming\"), (\"sexual\", \"identities\"),(\"sexual\", \"identity\"),\n",
    "(\"gender\", \"identities\")]\n",
    "\n",
    "affirming_movements = [(\"uniting\"), \"uca\"]\n",
    "\n",
    "affirming_movements_bigrams = [(\"uniting\", \"church\"), (\"mcc\", \"sydney\")]\n",
    "\n",
    "# Nonaffirming\n",
    "non_affirming_keywords = [\"homosexuality\", \"ssa\"]\n",
    "\n",
    "non_affirming_bigrams = [(\"traditional\", \"marriage\"), (\"biblical\",\"values\"),(\"god's\",\"design\"),\n",
    "(\"husband\",\"wife\"), (\"same\", \"sex\"), (\"sex\", \"attraction\"), (\"biological\", \"sex\"), (\"biblical\", \"view\"),\n",
    "(\"institution\", \"marriage\"), (\"husbands\", \"wives\"), (\"one\", \"woman\"), (\"man\",\"woman\")]\n",
    "\n",
    "non_affirming_trigrams = [(\"biblical\", \"view\", \"marriage\"), (\"same\", \"sex\", \"attracition\")]\n",
    "\n",
    "non_affirming_movements = [\"acc\", \"www.acc.org.au\", \"hillsong\", \"presbyterian.org.au\", \"presbyterian\"]\n",
    "\n",
    "non_affirming_movements_trigrams = [(\"australian\", \"christian\", \"churches\"), (\"australian\", \"christian\", \"church\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Flagging Movements/Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network flagging function to include evidence\n",
    "def flag_affirming_or_non_affirming_network_with_evidence(row):\n",
    "    affirming_movement_evidence = []\n",
    "    non_affirming_movement_evidence = []\n",
    "    detected_movement = \"\"  # To store movement name\n",
    "\n",
    "    # Check for affirming movements/networks\n",
    "    for word in row[\"Tokens\"]:\n",
    "        if word in affirming_movements:\n",
    "            affirming_movement_evidence.append(word)\n",
    "            detected_movement = word  # Capture movement name\n",
    "\n",
    "    for bigram in row[\"Bigrams\"]:\n",
    "        if bigram in affirming_movements_bigrams:\n",
    "            affirming_movement_evidence.append(\" \".join(bigram))\n",
    "            detected_movement = \" \".join(bigram)  # Capture movement name\n",
    "\n",
    "    # Check for non-affirming movements/networks\n",
    "    for word in row[\"Tokens\"]:\n",
    "        if word in non_affirming_movements:\n",
    "            non_affirming_movement_evidence.append(word)\n",
    "            detected_movement = word  # Capture movement name\n",
    "\n",
    "    for trigram in row[\"Trigrams\"]:\n",
    "        if trigram in non_affirming_movements_trigrams:\n",
    "            non_affirming_movement_evidence.append(\" \".join(trigram))\n",
    "            detected_movement = \" \".join(trigram)  # Capture movement name\n",
    "\n",
    "    # Determine the flag and evidence\n",
    "    if affirming_movement_evidence:\n",
    "        return \"Affirming\", \", \".join(affirming_movement_evidence), detected_movement  # Affirming with evidence\n",
    "    elif non_affirming_movement_evidence:\n",
    "        return \"Non-Affirming\", \", \".join(non_affirming_movement_evidence), detected_movement  # Non-Affirming with evidence\n",
    "    else:\n",
    "        return \"Unknown\", \"\", \"\"  # Neither found\n",
    "\n",
    "# Apply the updated function\n",
    "df_cleaned[[\"AffirmingFlag\", \"Evidence\", \"Movement\"]] = df_cleaned.apply(\n",
    "    lambda row: pd.Series(flag_affirming_or_non_affirming_network_with_evidence(row)), axis=1\n",
    ")\n",
    "\n",
    "# Display results\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Flagging Affirming and Non-Affirming Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with evidence, saved to 'scraped_cleaned_data_with_evidence.csv'.\n",
      "Updated dataset with evidence and removed unknowns, saved to 'scraped_cleaned_data_with_evidence.csv_no_unknowns'.\n"
     ]
    }
   ],
   "source": [
    "# Flagging function to include evidence\n",
    "def flag_affirming_or_non_affirming_with_evidence(row):\n",
    "    affirming_evidence = []\n",
    "    non_affirming_evidence = []\n",
    "\n",
    "    # Check for affirming keywords or bigrams\n",
    "    for word in row[\"Tokens\"]:\n",
    "        if word in affirming_keywords:\n",
    "            affirming_evidence.append(word)\n",
    "    for bigram in row[\"Bigrams\"]:\n",
    "        if bigram in affirming_bigrams:\n",
    "            affirming_evidence.append(\" \".join(bigram))\n",
    "    \n",
    "    # Check for non-affirming keywords, bigrams, or trigrams\n",
    "    for word in row[\"Tokens\"]:\n",
    "        if word in non_affirming_keywords:\n",
    "            non_affirming_evidence.append(word)\n",
    "    for bigram in row[\"Bigrams\"]:\n",
    "        if bigram in non_affirming_bigrams:\n",
    "            non_affirming_evidence.append(\" \".join(bigram))\n",
    "    for trigram in row[\"Trigrams\"]:\n",
    "        if trigram in non_affirming_trigrams:\n",
    "            non_affirming_evidence.append(\" \".join(trigram))\n",
    "    \n",
    "    # Determine the flag and evidence\n",
    "    if affirming_evidence:\n",
    "        return \"Affirming\", \", \".join(affirming_evidence)  # Affirming with evidence\n",
    "    elif non_affirming_evidence:\n",
    "        return \"Non-Affirming\", \", \".join(non_affirming_evidence)  # Non-Affirming with evidence\n",
    "    else:\n",
    "        return \"Unknown\", \"\"  # Neither found\n",
    "\n",
    "# Apply the updated function\n",
    "df_cleaned[[\"AffirmingFlag\", \"Evidence\"]] = df_cleaned.apply(\n",
    "    lambda row: pd.Series(flag_affirming_or_non_affirming_with_evidence(row)), axis=1\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "df_cleaned.to_csv(\"scraped_cleaned_data_with_evidence.csv\", index=False, encoding = \"utf-8\")\n",
    "\n",
    "print(\"Updated dataset with evidence, saved to 'scraped_cleaned_data_with_evidence.csv'.\")\n",
    "\n",
    "# Filter out rows with \"Unknown\" in the AffirmingFlag column\n",
    "filtered_df = df_cleaned[df_cleaned[\"AffirmingFlag\"] != \"Unknown\"]\n",
    "\n",
    "# Save to CSV\n",
    "filtered_df.to_csv(\"scraped_cleaned_data_with_evidence_no_unknowns.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Updated dataset with evidence and removed unknowns, saved to 'scraped_cleaned_data_with_evidence.csv_no_unknowns'.\")\n",
    "\n",
    "# Save to xlsx \n",
    "df_cleaned.to_excel(\"scraped_cleaned_data_with_evidence.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "filtered_df.to_excel(\"scraped_cleaned_data_with_evidence_no_unknowns.xlsx\", index=False, engine=\"openpyxl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Exploratory Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c1. Top 20 Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of tokens into a single list\n",
    "all_tokens = [token for tokens in df_cleaned[\"Tokens\"] for token in tokens]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Get the top 20 most common words\n",
    "top_words = word_counts.most_common(20)\n",
    "\n",
    "# Display as a bar chart\n",
    "words, counts = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 20 Most Common Words\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c2. Top 20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of bigrams into a single list\n",
    "all_bigrams = [bigram for bigrams in df_cleaned[\"Bigrams\"] for bigram in bigrams]\n",
    "\n",
    "# Count bigram frequencies\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "# Get the top 20 most common bigrams\n",
    "top_bigrams = bigram_counts.most_common(20)\n",
    "\n",
    "# Display as a bar chart\n",
    "bigrams, counts = zip(*top_bigrams)\n",
    "bigram_labels = [' '.join(bigram) for bigram in bigrams]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(bigram_labels, counts)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 20 Most Common Bigrams\")\n",
    "plt.xlabel(\"Bigrams\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffirmingFlag\n",
      "Non-Affirming    6\n",
      "Affirming        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"scraped_cleaned_data_with_evidence.csv\")\n",
    "\n",
    "# Drop unknown labels (since they don’t help in supervised learning)\n",
    "df = df[df[\"AffirmingFlag\"] != \"Unknown\"]\n",
    "\n",
    "# Check class distribution\n",
    "print(df[\"AffirmingFlag\"].value_counts())\n",
    "\n",
    "# Define features (text) and target (label)\n",
    "X = df[\"Text\"]   # Church website text\n",
    "y = df[\"AffirmingFlag\"]  # Affirming or Non-Affirming\n",
    "\n",
    "# Split into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (5, 5000)\n",
      "Test data shape: (2, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit to top 5000 features\n",
    "    stop_words=\"english\",  # Remove common words\n",
    "    ngram_range=(1,2)  # Include unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train & test data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Print shape of transformed data\n",
    "print(\"Train data shape:\", X_train_tfidf.shape)\n",
    "print(\"Test data shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train logistic regression model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Affirming       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Flagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Returns a score: >0 positive, <0 negative, 0 neutral\n",
    "\n",
    "# Apply sentiment analysis to cleaned text\n",
    "df_cleaned[\"SentimentScore\"] = df_cleaned[\"Text\"].apply(get_sentiment)\n",
    "\n",
    "# Categorize rows based on sentiment score\n",
    "def categorize_sentiment(score):\n",
    "    if score > 0.2:  # Adjust thresholds as needed\n",
    "        return \"Likely Affirming\"\n",
    "    elif score < -0.2:\n",
    "        return \"Likely Non-Affirming\"\n",
    "    else:\n",
    "        return \"Neutral/Unknown\"\n",
    "\n",
    "df_cleaned[\"SentimentCategory\"] = df_cleaned[\"SentimentScore\"].apply(categorize_sentiment)\n",
    "\n",
    "# Combine sentiment with affirming keyword flag for a final assessment\n",
    "def final_flag(row):\n",
    "    if row[\"AffirmingFlag\"]:\n",
    "        return \"Affirming\"\n",
    "    elif row[\"SentimentCategory\"] == \"Likely Non-Affirming\":\n",
    "        return \"Non-Affirming\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df_cleaned[\"FinalFlag\"] = df_cleaned.apply(final_flag, axis=1)\n",
    "\n",
    "# Save updated DataFrame to CSV\n",
    "output_file = \"scraped_cleaned_with_sentiment.csv\"\n",
    "df_cleaned.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF and K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"scraped_cleaned_data.csv\")\n",
    "\n",
    "# Combine Tokens and Bigrams into a single column for TF-IDF\n",
    "df[\"Combined\"] = df[\"Tokens\"].apply(lambda x: \" \".join(eval(x))) + \" \" + \\\n",
    "                 df[\"Bigrams\"].apply(lambda x: \" \".join([\"_\".join(bigram) for bigram in eval(x)]))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit to top 5000 features\n",
    "    stop_words=None,    # No need for stop words, as Tokens are already cleaned\n",
    "    ngram_range=(1, 1)  # Focus on unigrams (and bigrams if added earlier)\n",
    ")\n",
    "\n",
    "# Apply TF-IDF to the combined Tokens and Bigrams column\n",
    "X_tfidf = tfidf.fit_transform(df[\"Combined\"])\n",
    "\n",
    "# View feature names for reference\n",
    "print(f\"Number of TF-IDF Features: {len(tfidf.get_feature_names_out())}\")\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize and fit K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # Adjust `n_clusters` as needed\n",
    "df[\"Cluster\"] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Analyze cluster distribution\n",
    "print(df[\"Cluster\"].value_counts())\n",
    "\n",
    "# Save clusters or predictions back to the dataset\n",
    "df.to_csv(\"updated_church_data.csv\", index=False)\n",
    "\n",
    "# Save TF-IDF matrix for future use\n",
    "from scipy.sparse import save_npz\n",
    "save_npz(\"tfidf_matrix.npz\", X_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "This section collapses the results of the sentiment analysis for each website. The end result is a table where each row is an individual church with their name, website, address, coordinates, whether they are affirming or non-affirming, and the evidence (a url) where they state their affirmation or non-affirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Testing if there are church's without their type set as church\n",
    "# API URLs\n",
    "base_url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "details_base_url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "\n",
    "# Function to get data from Google Places API with pagination\n",
    "def get_places_data(api_key, query, location, radius, type):\n",
    "    all_results = []  # To store all results across pages\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"query\": query, \n",
    "        \"location\": location,\n",
    "        \"radius\": radius,\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the response\n",
    "        data = response.json()\n",
    "        all_results.extend(data.get(\"results\", []))  # Add results from the current page\n",
    "        \n",
    "        # Check if there is a next page\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break  # No more pages, exit loop\n",
    "        \n",
    "        # Wait a few seconds before using the next_page_token to avoid request denial\n",
    "        import time\n",
    "        time.sleep(3)  # Google requires a short delay before using the next page token\n",
    "        \n",
    "        print(f\"Fetched {len(data.get('results', []))} results\")\n",
    "        print(f\"Next Page Token: {next_page_token}\")\n",
    "\n",
    "        # Update params with the next_page_token\n",
    "        params.update({\"pagetoken\": next_page_token})\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Function to get website URLs from Place Details API\n",
    "def get_place_details(api_key, place_id):\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"website,types\",\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    response = requests.get(details_base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        details_data = response.json().get(\"result\", {})\n",
    "        return details_data.get(\"website\", \"N/A\"), details_data.get(\"types\", [\"N/A\"])\n",
    "    else:\n",
    "        print(f\"Error fetching details for place_id {place_id}: {response.status_code}\")\n",
    "        return \"N/A\", [\"N/A\"]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "\n",
    "    for location in locations:\n",
    "        print(f\"Fetching data for location: {location}\")\n",
    "        places_data = get_places_data(API_KEY, query, location, radius, type)\n",
    "\n",
    "        for place in places_data:\n",
    "            name = place.get(\"name\", \"Unknown\")\n",
    "            place_id = place.get(\"place_id\")\n",
    "            \n",
    "            if place_id:\n",
    "                website, place_types = get_place_details(API_KEY, place_id)\n",
    "                place_types_str = \", \".join(place_types)  # Convert list to string for better readability\n",
    "                \n",
    "               # Store all results (NO filtering)\n",
    "                results.append([name, website, place_types_str])\n",
    "\n",
    "      # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results, columns=[\"Church Name\", \"Website\", \"Types\"])\n",
    "    \n",
    "    # Print results as a table in console\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv(\"churches_without_church_type.csv\", index=False)\n",
    "    \n",
    "    print(\"Results saved to 'churches_without_church_type.csv'\")\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
